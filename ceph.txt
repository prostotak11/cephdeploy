#ceph commads
ceph -s
* ceph health detail
* ceph osd pool ls detail
* ceph osd tree
* ceph osd df tree
* ceph versions

<<<<<<< HEAD
=======

#ceph bootstrap cluster


>>>>>>> 15f3052e0adaa10a1d8ad6b17b48bd8d5ddf8439
#find out id of osd/mon/mgr etc.
/var/lib/ceph/{cluster-id}/{service-name}.{service-id}

#remove mon
ceph mon remove customstore2

<<<<<<< HEAD
=======
#remove mgr
ceph orch daemon rm mgr.cephosd02.yymjpi #to find out mon id need to check name at ceph -s command


>>>>>>> 15f3052e0adaa10a1d8ad6b17b48bd8d5ddf8439
#fixed locked partitions
dmsetup info # check mapping
dmsetup remove ceph--32233844--166e--48e8--a6e0--38a6d8c4434e-osd--block--6e0b5a72--5c07--4be9--8886--f8d4b61003aa

#fixed incorrect error 5 (mbr to gpt)
sgdisk --zap-all /dev/sdb

#remove osd
ceph osd rm 1 (osd-id)


#get config
ceph config show {service-name}.{service-id}
#get full config
ceph config show-with-defaults {service-name}.{service-id}


#get replication factor of pool
ceph osd pool get {pool-name} size
#change replication factor of pool
ceph osd pool set {pool-name} size {number}

#get crushmap (ceph-base package must be installed)
ceph osd getcrushmap -o /{path}/{filename}
#decompile crushmap file
crushtool -d /{path}/{filename} -o /{path}/{filename}
#compile file
crushtool -c /{path}/{filename} -o /{path}/{filename}
#apply new changes in crushmap
ceph osd setcrushmap -i /{path}/{filename}

#auto reweight osd
ceph osd reweight-by-utilization

#create new pool
ceph osd pool create {name} {pg_num} {pgp_num} #pgp_num equals to pg_num

#manual reweight
ceph osd  crush reweight osd.{id} {number}

#delete pool
ceph osd pool delete --yes-i-really-really-mean-it {pool_name} {pool_name}

#ceph autoscaler status
ceph osd pool autoscale-status

#install public SSH key to a new host
ssh-copy-id -f -i /etc/ceph/ceph.pub root@customstore3

#add new host to a cluster
ceph orch host add *newhost*
#ceph bootstrap without mon
ceph orch apply mon --unmanaged

#innitialize rbd modbrobe
modprobe rbd
# enable application (rbd,cephfs,rgw)
ceph osd pool application enable DApool rbd(application type)

# get keyring
ceph auth get client.admin

#rbd create
rbd create foo --size 1G --image-feature layering -m 192.168.168.48  -p DApool

#rbd pools on clients node
rbd ls -p DApool

#rbd map
sudo rbd map -p DApool bar
rbd showmapped

#modify ceph dashboard password
ceph dashboard ac-user-set-password <username> <password>

#remove cluster
cepadm rm-cluster

#ssh key copy
ssh-copy-id -f -i /etc/ceph/ceph.pub root@customstore3

<<<<<<< HEAD
#add monitor
ceph orch daemon add mon --placement="customstore3:[v2:192.168.168.50:3300,v1:192.168.168.50:6789]"

=======

#to manual managing all cluster need this option
ceph orch apply {name of service} --unmanaged #{name of service} - mon, mgr, etc.

#add monitor
ceph orch daemon add mon --placement="customstore3:[v2:192.168.168.50:3300,v1:192.168.168.50:6789]"

#add mgr
ceph orch daemon add mgr cephosd02

>>>>>>> 15f3052e0adaa10a1d8ad6b17b48bd8d5ddf8439
#create mds
ceph orch apply mds Hyudai --placement="1 customstore4"


#cephfs
ceph osd pool create cephfs_data
ceph osd pool create cephfs_metadata
ceph fs new Hyudai cephfs_metadata cephfs_data
ceph fs ls
ceph mds stat

#mount cephfs on client node
mount -t ceph 192.168.168.48:6789,192.168.168.49:6789:/ /mnt/defstore -o name=foo,secretfile=/root/ceph.key
<<<<<<< HEAD

Create an additional user S3 key:

radosgw-admin key create --uid=lucidlink --key-type=s3 --access-key <access-key> --secret-key <secret-key>
note: you may create multiple S3 key pairs for a user. you may also create multiple subusers and S3 key pairs.
Remove a key:

radosgw-admin key rm --uid=lucidlink --key-type=s3 --access-key <access-key>
=======
>>>>>>> 15f3052e0adaa10a1d8ad6b17b48bd8d5ddf8439

Rename pool:
ceph osd pool rename {current-pool-name} {new-pool-name}
